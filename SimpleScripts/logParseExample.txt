import org.apache.spark.sql.{SparkSession, SaveMode, DataFrameReader, Row, Dataset, Column}

object Main {
  def main(args: Array[String]) {

    val spark = SparkSession.builder.appName("mumbleLogBatchParser").getOrCreate()

    /**Importing implicits class and functions class**/
    import spark.implicits._
    import org.apache.spark.sql.functions._

    /** Initializing regex for pattern matching **/
    val datePtrn = """([0-9]{4}-[0-9]{2}-[0-9]{2})""".r
    val logIDPtrn = """( => <\d+:)""".r
    val usernamePtrn = """(\w+\(.+\))""".r
    val versionPtrn = """(\(\w+\:\s\d+\.\d+\.\d+\)|\(\w+\:\s\w+\s\w*\s\d+\.\d+\.\d\))""".r
    val ipAddressPtrn = """(\\d+\\.\\d+\\.\\d+\\.\\d+)""".r
    val timestampPtrn = """([0-9]{4}-[0-9]{2}-[0-9]{2}\s[0-9]{2}:[0-9]{2}:[0-9]{2}.[0-9]{3})""".r

    val filepath = "/home/ross/Downloads/"


    val log = spark.read.textFile(filepath + "murmur.log")

    /** Filters the logfile first so that it only includes rows we want for now, then applies map to it **/

    val logMap = log.filter(line => line.contains("New connection")
                    || line.contains("Connection closed")
                    || line.contains("Authenticated")
                    || line.contains("version")
    )
                    .map{line => (datePtrn.findFirstIn(line).getOrElse(null)
                        ,logIDPtrn.findFirstIn(line).getOrElse(null)
                        ,usernamePtrn.findFirstIn(line).getOrElse(null)
                        ,versionPtrn.findFirstIn(line).getOrElse(null)
                        ,ipAddressPtrn.findFirstIn(line).getOrElse(null)
                        ,timestampPtrn.findFirstIn(line).getOrElse(null))
                    }

    /**Renames the columns of the dataset**/
    val newNames = Seq("date","logID","username","version","ipAddr","timestamp")
    val logRenamed = logMap.toDF(newNames: _*)

    /**Does the groupBy and aggregation to remove the nulls and get one row instead of 3-4**/
    val logGrouped = logRenamed.groupBy($"date",$"logID")
                               .agg( max($"username")
                                    ,max($"version")
                                    ,max($"ipAddr")
                                    ,min($"timestamp")
                                    ,max($"timestamp"))

    /**Another rename of columns, with a sort to cap it off**/
    val newNames2 = Seq("date","logID","username","version","ipAddr","logOnTime","logOffTime")
    val logGroupedRenamed = logGrouped.toDF(newNames2: _*)



    /**A data quality map that applies a function to some of the columns so that I can actually get less messy data**/
    val logDQ = logGroupedRenamed.withColumn("logID",regexp_extract($"logID","""(\d+)""",1))

    /**Sorts and coalesces to a single file**/
    val logFinal = logDQ.sort($"logOnTime",$"logID").repartition(1)



    /**Export to a file, probably should make this a table insert since that'd make more sense**/

    logFinal.write.mode(SaveMode.Overwrite).json(filepath + "/murmurFilter")


    spark.stop()
  }
}